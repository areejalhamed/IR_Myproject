#m.py
import ir_datasets
from tqdm import tqdm
from database import setup_documents_table, upload_docs_with_cleaning, get_existing_doc_ids

def process_dataset(dataset_path, table_name):
    print(f"â¬†ï¸ ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© {dataset_path}...")
    ds = ir_datasets.load(dataset_path)

    count = 0
    for doc in tqdm(ds.docs_iter(), desc=f"ğŸ§¾ Ù‚Ø±Ø§Ø¡Ø© Ù…Ø³ØªÙ†Ø¯Ø§Øª {dataset_path}"):
        try:
            _ = doc.text
            count += 1
        except UnicodeDecodeError:
            print(f"âš ï¸ ØªØ®Ø·ÙŠ Ù…Ø³ØªÙ†Ø¯ Ù„ÙˆØ¬ÙˆØ¯ Ø®Ø·Ø£ ØªØ±Ù…ÙŠØ²: {doc.doc_id}")

    print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù‚Ø±ÙˆØ¡Ø© Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ {dataset_path}: {count}")
    #Ù„Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù†Ø¸ÙØ© Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø®Ø§Øµ ÙÙŠÙ‡Ø§
    upload_docs_with_cleaning(table_name, ds)

def main():
    print("ğŸš€ <<< Start >>>")

    datasets = [
        ("beir/quora/dev", "documents_beir_quora_dev"),
        ("antique/test", "documents_antique_test")
    ]
    
    for dataset_path, table_name in datasets:
        print(f"ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ø¯ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {table_name}")

        #Ø§Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© 
        setup_documents_table(table_name)

        #Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ø§ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ 
        existing_ids = get_existing_doc_ids(table_name)
        if existing_ids:
            #Ù‡Ù†Ø§ Ù†ØªØ®Ø·Ù‰ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙÙŠ Ø­Ø§Ù„ ÙƒØ§Ù† ÙŠÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø± 
            print(f"â­ï¸ ÙˆØ¬Ø¯Ù†Ø§ {len(existing_ids)} Ù…Ø³ØªÙ†Ø¯ ÙÙŠ {table_name}ØŒ Ù„Ø°Ù„Ùƒ Ù„Ù† Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„.")
        else:
            #Ù†Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª 
            process_dataset(dataset_path, table_name)

if __name__ == "__main__":
    main()
